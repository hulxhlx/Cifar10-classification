{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TVP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGirSnwaCQxf",
        "outputId": "b178f22e-55ff-4204-ce42-9980295be2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NvqerCrDfai",
        "outputId": "09aba321-8af2-4b1a-a032-0fd6d9291a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/PVT-tensorflow2-main\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnaqOHmbCuPJ",
        "outputId": "03ebf88b-4e5a-4804-a2ba-b3f8ee90181e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.gitignore',\n",
              " 'README.md',\n",
              " 'model',\n",
              " '.idea',\n",
              " 'generator',\n",
              " 'saved_models',\n",
              " 'dataset',\n",
              " 'checkpoints',\n",
              " 'utils',\n",
              " 'requirements.txt',\n",
              " '.ipynb_checkpoints',\n",
              " '1.png',\n",
              " '2.png',\n",
              " 'transformer.h5',\n",
              " 'train.py',\n",
              " '10605.png',\n",
              " '20605.png',\n",
              " 'pvt0605.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yGr3q50AUSrC",
        "outputId": "d2cbee0f-3ae1-4716-ab2c-48c252dbef6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.2+zzzcolab20220527125636.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 16 kB/s \n",
            "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 41.2 MB/s \n",
            "\u001b[?25hCollecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 21.0 MB/s \n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.8.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68713 sha256=88127814f95dd132fd30de3e2c3e849eecdc96ee740a7f32b25a52186827aa12\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.3\n",
            "    Uninstalling grpcio-1.46.3:\n",
            "      Successfully uninstalled grpcio-1.46.3\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSNfcUjvSvs7",
        "outputId": "fa6a574b-fcec-4146-f69e-3e6520625e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %matplotlib inline\n",
        "# %matplotlib notebook\n",
        "\n",
        "# !python train.py"
      ],
      "metadata": {
        "id": "W-wEoTSiDD6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generator():\n",
        "    train_generator = CIFARGenerator( mode=\"train\")\n",
        "    val_generator = CIFARGenerator( mode=\"valid\")\n",
        "    return train_generator, val_generator"
      ],
      "metadata": {
        "id": "kHL3sh2-jq3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def get_optimizers():\n",
        "    warmup_epochs =0\n",
        "    warmup_lr = 1e-5\n",
        "    init_lr = 1e-4\n",
        "    weight_decay=0\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=init_lr,name='adam',weight_decay=weight_decay)\n",
        "\n",
        "    return optimizer\n"
      ],
      "metadata": {
        "id": "bFjRVUKOmNyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def get_lr_scheduler(lr_scheduler):\n",
        "    warmup_epochs =0\n",
        "    warmup_lr = 1e-5\n",
        "    init_lr = 1e-4\n",
        "    if lr_scheduler == 'step':\n",
        "        def scheduler(epoch,lr=0.001):\n",
        "            if epoch < warmup_epochs:\n",
        "                current_epoch_lr = warmup_lr + epoch * (init_lr -warmup_lr) / warmup_epochs\n",
        "                print(\"epoch:{} lr:{:.6f}\".format(epoch,current_epoch_lr.numpy()))\n",
        "                return current_epoch_lr\n",
        "            else:\n",
        "                for index, val in enumerate(lr_decay_epoch):\n",
        "                    if epoch < val:\n",
        "                        current_epoch_lr = init_lr*lr_decay**index\n",
        "                        print(\"epoch:{} lr:{:.6f}\".format(epoch,current_epoch_lr.numpy()))\n",
        "                        return current_epoch_lr\n",
        "            current_epoch_lr = init_lr*lr_decay**len(lr_decay_epoch)\n",
        "            print(\"epoch:{} lr:{:.6f}\".format(epoch,current_epoch_lr.numpy()))\n",
        "            return current_epoch_lr\n",
        "        return scheduler\n",
        "    elif lr_scheduler == 'cosine':\n",
        "        def scheduler(epoch,lr=0.001):\n",
        "            if epoch < warmup_epochs:\n",
        "                current_epoch_lr = warmup_lr + epoch * (init_lr - warmup_lr) / warmup_epochs\n",
        "            else:\n",
        "                current_epoch_lr = init_lr * (\n",
        "                        1.0 + tf.math.cos(np.pi / (epochs - warmup_epochs) * (epoch - warmup_epochs))) / 2.0\n",
        "            print(\"epoch:{} lr:{:.6f}\".format(epoch,current_epoch_lr.numpy()))\n",
        "            return current_epoch_lr\n",
        "    else:\n",
        "        raise ValueError(\"{} is not supported!\".format(lr_scheduler))\n",
        "    return scheduler\n"
      ],
      "metadata": {
        "id": "YfTTfsXlmyZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.efficientnet import EfficientNet\n",
        "from model.resnet import ResNet\n",
        "from model.PVT import PVTNet\n",
        "from model.VIT import VITNet\n",
        "from model.resnet_for_cifar import ResNetForCifar\n",
        "def get_model(mode,num_class):\n",
        "\n",
        "    model = PVTNet(img_size=32, classes=num_class,type=mode.split('-')[-1],pretrain=None)\n",
        "    return model.get_model()\n",
        "def get_model(mode,num_class):\n",
        "\n",
        "    model = VITNet(img_size=32, classes=num_class,type=mode.split('-')[-1],pretrain=None)\n",
        "    return model.get_model()\n"
      ],
      "metadata": {
        "id": "CiRVlaF-n6Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalization(train_images, test_images):\n",
        "    mean = np.mean(train_images, axis=(0, 1, 2, 3))\n",
        "    std = np.std(train_images, axis=(0, 1, 2, 3))\n",
        "    train_images = (train_images - mean) / (std + 1e-7)\n",
        "    test_images = (test_images - mean) / (std + 1e-7)\n",
        "    return train_images, test_images\n",
        "\n",
        "def load_images():\n",
        "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "    train_images = train_images.astype(np.float32)\n",
        "    test_images = test_images.astype(np.float32)\n",
        "\n",
        "    (train_images, test_images) = normalization(train_images, test_images)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 10)\n",
        "    test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "    # train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
        "    #     buffer_size=10000).batch(batch_size)\n",
        "    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "rm1Za7WtMpIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sb\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_images()\n",
        "datagen = ImageDataGenerator(\n",
        "\t\tfeaturewise_center=False,  # set input mean to 0 over the dataset\n",
        "\t\tsamplewise_center=False,  # set each sample mean to 0\n",
        "\t\tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "\t\tsamplewise_std_normalization=False,  # divide each input by its std\n",
        "\t\tzca_whitening=False,  # apply ZCA whitening\n",
        "\t\trotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "\t\twidth_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "\t\theight_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "\t\thorizontal_flip=True,  # randomly flip images\n",
        "\t\tvertical_flip=False)  # randomly flip images\n",
        "\t# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train)\n",
        "model = VITNet(img_size=32, classes=10,pretrain=None)\n",
        "model = model.get_model()\n",
        "# model = get_model('PVT-tiny',10)\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = get_optimizers()\n",
        "lr_scheduler = get_lr_scheduler('cosine')\n",
        "\n",
        "    # lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, verbose=1, min_lr=0)\n",
        "lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "model.compile(optimizer,loss_object,metrics=[\"accuracy\"])\n",
        "history = model.fit(x_train,y_train,\n",
        "                          validation_data=(x_test, y_test),\n",
        "                          epochs=200,batch_size=128,\n",
        "                          callbacks=[],\n",
        "                          verbose=1,\n",
        "                          )\n",
        "plt.plot(history.history[\"accuracy\"], label=\"training\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"validation\")\n",
        "plt.title(\"Googlenet\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.savefig ('10605.png') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
        "plt.title(\"Googlenet\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig ('20605.png') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "test_predict = model.predict(x_test)\n",
        "y_test_labels = np.argmax(test_predict, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_test, y_test_labels)\n",
        "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print(\"Confusion Matrix\\n\", cm)\n",
        "df_c_matrix = pd.DataFrame(cm, index=[clss for clss in class_labels], columns=[clss for clss in class_labels])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sb.heatmap(df_c_matrix, annot=True)\n",
        "plt.show()\n",
        "model.save_weights('pvt0605.h5')\n",
        "# model.load_weights('transformer.h5')\n"
      ],
      "metadata": {
        "id": "IGJA2B54Eqnw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "c84a30d1-4dee-4f82-8884-64df4a565e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bfe539a1220a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVITNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m# model = get_model('PVT-tiny',10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mloss_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VITNet' object has no attribute 'get_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, val_generator = get_generator()\n",
        "for i in range(10):\n",
        "  img = CIFARGenerator( mode=\"train\")\n",
        "\n"
      ],
      "metadata": {
        "id": "al4HPf-mH9nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(_, train_labels), (_, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "train_generator, val_generator = get_generator()\n",
        "train_images =  train_generator.trainimg()\n",
        "test_images =  val_generator.trainimg()\n",
        "xtrain_output = model.predict(train_images)\n"
      ],
      "metadata": {
        "id": "mk2LNoVAtDKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}